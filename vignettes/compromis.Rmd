---
title: "compromis"
author: "Alexandre CONANEC"
date: "31 mai 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r }
library(OptFilBov)

#import data
data <- as.matrix(read.csv('../data/big_simulation4.csv', header = T, sep = ';', dec = ',')[,-26])
initiation <- as.factor(as.vector(read.csv('../data/big_simulation3.csv', header = T, sep = ';', dec = ',')[,26]))
CDC <- read.csv('../data/cahier_des_charges2.csv', header = T, sep = ';', dec = ',')
CDC$objectif <- as.factor(CDC$objectif) 
CDC

objectifs <- list()
for (k in levels(CDC$objectif)){

  objectifs[[k]]$index <- which(CDC$objectif == k)
  objectifs[[k]]$w <- CDC$coef_agreg[which(CDC$objectif == k)]
  names(objectifs)[as.numeric(k)] <- paste(unlist(strsplit(as.character(CDC$cluster[which(CDC$objectif == k)][1]), split = ''))[-3], collapse = '')
  
}


constraint <- list(seuil_plus = CDC$seuil[which(! is.na(CDC$seuil) & CDC$seuil > 0)],
                   seuil_minus = CDC$seuil[which(! is.na(CDC$seuil) & CDC$seuil < 0)],
                   index_plus = which(! is.na(CDC$seuil) & CDC$seuil > 0),
                   index_minus = which(! is.na(CDC$seuil) & CDC$seuil < 0))



res <- bovCDC(X = data[,-1],
              objectifs = objectifs,
              constraint = constraint,
              scale = TRUE)

delete_by_threshold <- which(!res$constraint_respected)
res <- res[-delete_by_threshold, 1:2]
```

A cette étape on vient de supprimer `r length(delete_by_threshold)` individus qui ne respectait pas les conditions. A remarquer que c'est faible mais aussi que le seuil est en faite un plafond (donc l'inverse de ce que l'on recherche) pour les indicateurs qui sont optimiser lorsqu'il sont faibles.
```{r}
# svg(filename="../plot/compromis2.svg")

plot(res) 
lines(ksmooth(x = res[,1], y = res[,2]), col = 'red')



# dev.off()

X_scale <- scale(data)
X_decision <- pareto_finding (X = X_scale, Y = res, method = 'target', target = c(1.5,2.5), n = 2, plot = TRUE)


bon <- c(4462, 4497)
mauvais <- c(3907, 4374)
a <- rep(0, nrow(res))
a[bon] <- 1
a[mauvais] <- 2
a <- as.factor(a)

plot(res, col = c('grey', 'blue', 'red')[a])
dev.copy(png, '../plot/compromis.png')
dev.off()
```

Par rapport aux derniers resultats, on remarque que QS et QN sont lègerement negativement correles. Bon signe pour notre étude même si c'est pas flagrant et que les points du milieu sont un peu bizarres (cf l'explication plus bas) 
```{r}

# dev.copy(png,'../plot/compromis.png')
# dev.off()

for (i in 1:nrow(X_decision$X)){

  cow <- as.matrix(as.data.frame(t(X_decision$X[i,]))[-1])
  class(cow) <- 'cow'
  plot(cow, choice = 'radar_diag',  radial.lim = c(-4,4))
  
  dev.copy(png, paste('../plot/bon_profil', i, '.png'))
  dev.off()

}

X_decision <- pareto_finding (X = X_scale, Y = res, method = 'target', target = c(-1.5,-2.5), n = 2, plot = TRUE)

for (i in 1:nrow(X_decision$X)){

  cow <- as.matrix(as.data.frame(t(X_decision$X[i,]))[-1])
  class(cow) <- 'cow'
  plot(cow, choice = 'radar_diag',  radial.lim = c(-4,4))
  
  dev.copy(png, paste('../plot/mauvais_profil', i, '.png'))
  dev.off()

}
```

Ici on retrouve le profil de nos meilleurs compromis.

Je regarde maintenant quels sont les 'initiateurs' des points qui semblent disperses au milieu, car je soupçonne que ce soit majoritairement des QS. En effet je pense que la seule variabilité importante est donnée par des valeurs initiées au départ et non par les modeles.
```{r}
X_decision <- pareto_finding (X = X_scale, Y = res, method = 'weight_aggregation', w = c(0.2, 0.8), n = 350, plot = TRUE)

ind_sel <- as.numeric(rownames(X_decision$Y))
ind_sel_corrected <- c()
for (i in 1:length(ind_sel)){
  
  correction <- length(which(ind_sel[i] > delete_by_threshold))
  ind_sel_corrected[i] <- ind_sel[i] + correction
}

table(initiation[ind_sel_corrected])
```

Hypothese verifiée! On observe qu'une large majorité des valeurs (`r table(initiation[ind_sel_corrected])[4]`) en bleu ont été initié par QS (cad que l'on a fixé les indicateurs de QS et calculer les autres avec les modèles). Cela semble néanmoins, moins flagrant qu'avec les derniers résultats. Cela s'explique par le fait que seul la jutosité a un mauvais modèle (et predit en conséquence des valeurs moyennes) mais le poids accordé est faible.
